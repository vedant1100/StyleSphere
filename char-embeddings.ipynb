{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7752969,"sourceType":"datasetVersion","datasetId":4533195}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn","metadata":{"execution":{"iopub.status.busy":"2024-03-04T06:45:13.610299Z","iopub.execute_input":"2024-03-04T06:45:13.611272Z","iopub.status.idle":"2024-03-04T06:45:17.378882Z","shell.execute_reply.started":"2024-03-04T06:45:13.611228Z","shell.execute_reply":"2024-03-04T06:45:17.377785Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"class fixed_gru(nn.Module):\n    def __init__(self, num_steps, emb_dim):\n        super().__init__()\n        # update gate\n        self.i2h_update = nn.Linear(emb_dim, emb_dim)\n        self.h2h_update = nn.Linear(emb_dim, emb_dim)\n\n        # reset gate\n        self.i2h_reset = nn.Linear(emb_dim, emb_dim)\n        self.h2h_reset = nn.Linear(emb_dim, emb_dim)\n\n        # candidate hidden state\n        self.i2h = nn.Linear(emb_dim, emb_dim)\n        self.h2h = nn.Linear(emb_dim, emb_dim)\n\n        self.num_steps = num_steps\n\n\n    def forward(self, txt):\n        res = []\n        res_intermediate = []\n        for i in range(self.num_steps):\n            if i == 0:\n                output = torch.tanh(self.i2h(txt[:, i])).unsqueeze(1)\n            else:\n                # compute update and reset gates\n                update = torch.sigmoid(self.i2h_update(txt[:, i]) + self.h2h_update(res[i-1]))\n                reset = torch.sigmoid(self.i2h_reset(txt[:, i]) + self.h2h_reset(res[i-1]))\n\n                # compute candidate hidden state\n                gated_hidden = reset * res[i-1]\n                p1 = self.i2h(txt[:, i])\n                p2 = self.h2h(gated_hidden)\n                hidden_cand = torch.tanh(p1 + p2)\n\n                # use gates to interpolate hidden state\n                zh = update * hidden_cand\n                zhm1 = ((update * -1) + 1) * res[i-1]\n                output = zh + zhm1\n\n            res.append(output)\n            res_intermediate.append(output)\n\n        res = torch.cat(res, dim=1)\n        res = torch.mean(res, dim=1)\n        return res","metadata":{"id":"5S6DjV4MYOQ9","execution":{"iopub.status.busy":"2024-03-04T06:45:17.380475Z","iopub.execute_input":"2024-03-04T06:45:17.380849Z","iopub.status.idle":"2024-03-04T06:45:17.392514Z","shell.execute_reply.started":"2024-03-04T06:45:17.380824Z","shell.execute_reply":"2024-03-04T06:45:17.391644Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class char_cnn_rnn(nn.Module):\n    def __init__(self):\n        super().__init__()\n        rnn=fixed_gru\n        use_maxpool3=True\n        self.use_maxpool3=use_maxpool3\n        rnn_num_steps=8\n        rnn_dim=512\n\n        # network setup\n        # (B, 70, 201)\n        self.conv1 = nn.Conv1d(70, 384, kernel_size=4)\n        self.threshold1 = nn.Threshold(1e-6, 0)\n        self.maxpool1 = nn.MaxPool1d(kernel_size=3, stride=3)\n        # (B, 384, 66)\n        self.conv2 = nn.Conv1d(384, 512, kernel_size=4)\n        self.threshold2 = nn.Threshold(1e-6, 0)\n        self.maxpool2 = nn.MaxPool1d(kernel_size=3, stride=3)\n        # (B, 512, 21)\n        self.conv3 = nn.Conv1d(512, rnn_dim, kernel_size=4)\n        self.threshold3 = nn.Threshold(1e-6, 0)\n        if use_maxpool3:\n            self.maxpool3 = nn.MaxPool1d(kernel_size=3,stride=2)\n        # (B, rnn_dim, rnn_num_steps)\n        self.rnn = rnn(num_steps=rnn_num_steps, emb_dim=rnn_dim)\n        # (B, rnn_dim)\n        self.emb_proj = nn.Linear(rnn_dim, 1024)\n        # (B, 1024)\n\n\n    def forward(self, txt):\n        # temporal convolutions\n#         print(txt.shape)\n        out = self.conv1(txt)\n        out = self.threshold1(out)\n        out = self.maxpool1(out)\n\n        out = self.conv2(out)\n        out = self.threshold2(out)\n        out = self.maxpool2(out)\n\n        out = self.conv3(out)\n        out = self.threshold3(out)\n        if self.use_maxpool3:\n            out = self.maxpool3(out)\n\n        # recurrent computation\n        out = out.permute(0, 2, 1)\n        out = self.rnn(out)\n\n        # linear projection\n        out = self.emb_proj(out)\n\n        return out","metadata":{"id":"DcsfetReYR_y","execution":{"iopub.status.busy":"2024-03-04T06:45:17.393763Z","iopub.execute_input":"2024-03-04T06:45:17.394193Z","iopub.status.idle":"2024-03-04T06:45:17.407818Z","shell.execute_reply.started":"2024-03-04T06:45:17.394162Z","shell.execute_reply":"2024-03-04T06:45:17.407044Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def prepare_text(string, max_str_len=201):\n    '''\n    Converts a text description from string format to one-hot tensor format.\n    '''\n    labels = str_to_labelvec(string, max_str_len)\n    one_hot = labelvec_to_onehot(labels)\n    return one_hot\n\n\n\ndef str_to_labelvec(string, max_str_len):\n    string = string.lower()\n    alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{} \"\n    alpha_to_num = {k:v+1 for k,v in zip(alphabet, range(len(alphabet)))}\n    labels = torch.zeros(max_str_len).long()\n    max_i = min(max_str_len, len(string))\n    for i in range(max_i):\n        labels[i] = alpha_to_num.get(string[i], alpha_to_num[' '])\n\n    return labels\n\n\n\ndef labelvec_to_onehot(labels):\n    labels2 = torch.LongTensor(labels).unsqueeze(0)\n#     print(labels2.shape)\n    one_hot = torch.zeros(labels2.size(1), 71).scatter_(1, labels2, 1.)\n    # ignore zeros in one-hot mask (position 0 = empty one-hot)\n    one_hot = one_hot[:, 1:]\n    one_hot = one_hot.permute(1,0)\n#     print(one_hot.shape)\n    return one_hot\n\n\n\ndef onehot_to_labelvec(tensor):\n    labels = torch.zeros(tensor.size(1), dtype=torch.long)\n    val, idx = torch.nonzero(tensor).split(1, dim=1)\n    labels[idx] = val+1\n    return labels\n\n\n\ndef labelvec_to_str(labels):\n    '''\n    Converts a text description from one-hot tensor format to string format.\n    '''\n    alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{} \"\n    string = [alphabet[x-1] for x in labels if x > 0]\n    string = ''.join(string)\n    return string","metadata":{"id":"dm17jQ8vYXWd","execution":{"iopub.status.busy":"2024-03-04T11:42:31.793160Z","iopub.execute_input":"2024-03-04T11:42:31.793944Z","iopub.status.idle":"2024-03-04T11:42:31.804683Z","shell.execute_reply.started":"2024-03-04T11:42:31.793909Z","shell.execute_reply":"2024-03-04T11:42:31.803799Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"import random","metadata":{"id":"DuzBVWx5aGpi","execution":{"iopub.status.busy":"2024-03-04T11:33:47.221532Z","iopub.execute_input":"2024-03-04T11:33:47.221922Z","iopub.status.idle":"2024-03-04T11:33:47.227737Z","shell.execute_reply.started":"2024-03-04T11:33:47.221884Z","shell.execute_reply":"2024-03-04T11:33:47.225403Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"def rng_init(seed):\n    random.seed(seed)\n    #np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\n\n\ndef init_weights(m):\n    if isinstance(m, torch.nn.Linear):\n        torch.nn.init.uniform_(m.weight, a=-0.08, b=0.08)\n        torch.nn.init.uniform_(m.bias, a=-0.08, b=0.08)\n    elif isinstance(m, torch.nn.Conv1d):\n        torch.nn.init.uniform_(m.weight, a=-0.08, b=0.08)\n        torch.nn.init.uniform_(m.bias, a=-0.08, b=0.08)","metadata":{"id":"W0iLfqCPZsOQ","execution":{"iopub.status.busy":"2024-03-04T06:45:17.434977Z","iopub.execute_input":"2024-03-04T06:45:17.435292Z","iopub.status.idle":"2024-03-04T06:45:17.443617Z","shell.execute_reply.started":"2024-03-04T06:45:17.435263Z","shell.execute_reply":"2024-03-04T06:45:17.442920Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import argparse\nfrom collections import OrderedDict\n\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom datetime import datetime\nfrom torch.utils.tensorboard import SummaryWriter","metadata":{"id":"rh8SPe0UaV32","execution":{"iopub.status.busy":"2024-03-04T06:45:17.444609Z","iopub.execute_input":"2024-03-04T06:45:17.444899Z","iopub.status.idle":"2024-03-04T06:45:29.229819Z","shell.execute_reply.started":"2024-03-04T06:45:17.444857Z","shell.execute_reply":"2024-03-04T06:45:29.228816Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"2024-03-04 06:45:19.578013: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-04 06:45:19.578114: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-04 06:45:19.711343: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"class MultimodalDataset(Dataset):\n    '''\n    Preprocessed Caltech-UCSD Birds 200-2011 and Oxford 102 Category Flowers\n    datasets, used in ``Learning Deep Representations of Fine-grained Visual\n    Descriptions``.\n\n    Download data from: https://github.com/reedscot/cvpr2016.\n\n    Arguments:\n        data_dir (string): path to directory containing dataset files.\n        split (string): which data split to load.\n    '''\n    def __init__(self, data_dir):\n        super().__init__()\n        self.split_classes = ['dresses','jackets and coats','jeans','pants','shirts','shorts','skirts','suits and blazers','sweaters','tops']\n        self.nclass = len(self.split_classes)\n\n        self.data = {}\n        self.num_instances = 0\n        for cls in self.split_classes:\n#             print(cls)\n            path_imgs=f'{data_dir}/img-emb/img-emb/train/{cls}.t7'\n            path_txts=f'{data_dir}/text-to-label/text-to-label/train/{cls}.t7'\n            cls_imgs = torch.Tensor(torch.load(path_imgs))\n            cls_txts = torch.LongTensor(torch.load(path_txts))\n#             print(cls_imgs.size,cls_txts.size)\n            self.data[cls] = (cls_imgs, cls_txts)\n\n            self.num_instances += cls_imgs.size(0)\n\n\n    def __len__(self):\n        # WARNING: this number is somewhat arbitrary, since we do not\n        # necessarily use all instances in an epoch\n        return self.num_instances\n\n\n    def __getitem__(self, index):\n        cls_id = torch.randint(self.nclass, (1,))\n        cls = self.split_classes[cls_id]\n        cls_imgs, cls_txts = self.data[cls]\n\n#         id_txt = torch.randint(cls_txts.size(2), (1,))\n#         id_instance = torch.randint(cls_txts.size(0), (1,))\n#         id_view = torch.randint(cls_imgs.size(2), (1,))\n\n#         img = cls_imgs[id_instance, :, id_view].squeeze()\n#         txt = cls_txts[id_instance, :, id_txt].squeeze()\n        \n        id=torch.randint(cls_txts.size(0),(1,))\n#         id_view=torch.randint(cls_imgs.size(0),(1,))\n        img = cls_imgs[id]\n        txt = cls_txts[id]\n        \n        txt = labelvec_to_onehot(txt)\n\n        return {'img': img, 'txt': txt}","metadata":{"id":"2Po3vUcUroUz","execution":{"iopub.status.busy":"2024-03-04T09:37:15.653636Z","iopub.execute_input":"2024-03-04T09:37:15.654034Z","iopub.status.idle":"2024-03-04T09:37:15.664594Z","shell.execute_reply.started":"2024-03-04T09:37:15.654002Z","shell.execute_reply":"2024-03-04T09:37:15.663693Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"def sje_loss(feat1, feat2):\n    ''' Structured Joint Embedding Loss '''\n    # similarity score matrix (rows: fixed feat2, columns: fixed feat1)\n    scores = torch.matmul(feat2, feat1.t()) # (B, B)\n    # diagonal: matching pairs\n    diagonal = scores.diag().view(scores.size(0), 1) # (B, 1)\n    # repeat diagonal scores on rows\n    diagonal = diagonal.expand_as(scores) # (B, B)\n    # calculate costs\n    cost = (1 + scores - diagonal).clamp(min=0) # (B, B)\n    # clear diagonals (matching pairs are not used in loss computation)\n    cost[torch.eye(cost.size(0)).bool()] = 0 # (B, B) for torch==1.2.0\n#     cost[torch.eye(cost.size(0), dtype=torch.uint8)] = 0 # (B, B)\n    # sum and average costs\n    denom = cost.size(0) * cost.size(1)\n    loss = cost.sum() / denom\n\n    # batch accuracy\n    max_ids = torch.argmax(scores, dim=1)\n    ground_truths = torch.LongTensor(range(scores.size(0))).to(feat1.device)\n    num_correct = (max_ids == ground_truths).sum().float()\n    accuracy = 100 * num_correct / cost.size(0)\n\n    return loss, accuracy","metadata":{"id":"MBqeu-lzqngl","execution":{"iopub.status.busy":"2024-03-04T09:37:17.540242Z","iopub.execute_input":"2024-03-04T09:37:17.540945Z","iopub.status.idle":"2024-03-04T09:37:17.549915Z","shell.execute_reply.started":"2024-03-04T09:37:17.540911Z","shell.execute_reply":"2024-03-04T09:37:17.548760Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def main():\n    rng_init(42)\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    data_dir='/kaggle/input/embeddings/'\n    dataset = MultimodalDataset(data_dir)\n    loader = DataLoader(dataset, batch_size=50, shuffle=True,\n            num_workers=1, pin_memory=True)\n    loader_len = len(loader)\n#     print(\"Done dataset\")\n\n    os.makedirs('/kaggle/working/checkpoints', exist_ok=True)\n    timestamp = datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n    model_name = '{}_{:.5f}_{}_{}_{}.pth'.format('checkpoint',\n            0.0004, True, 'train', timestamp)\n    ckpt_path = os.path.join('/kaggle/working/checkpoints', model_name)\n    writer = SummaryWriter('/kaggle/working/checkpoints')\n\n    net_txt = char_cnn_rnn().to(device)\n    first_time=True\n    net_txt.apply(init_weights)\n\n    optim_txt = torch.optim.RMSprop(net_txt.parameters(), lr=0.0004)\n    sched_txt = torch.optim.lr_scheduler.ExponentialLR(optim_txt,0.98)\n\n    acc1_smooth = acc2_smooth = 0\n\n    for epoch in tqdm(range(300), position=1):\n        for i, data in enumerate(tqdm(loader, position=0)):\n            iter_num = (epoch * loader_len) + i + 1\n\n            net_txt.train()\n            img = data['img'].squeeze().to(device)\n            txt = data['txt'].to(device)\n            feat_txt = net_txt(txt)\n            feat_img = img\n#             print(feat_img.shape)\n\n            loss1, acc1 = sje_loss(feat_txt, feat_img)\n            loss2 = acc2 = 0\n            loss2, acc2 = sje_loss(feat_img, feat_txt)\n            loss = loss1 + loss2\n\n            acc1_smooth = 0.99 * acc1_smooth + 0.01 * acc1\n            acc2_smooth = 0.99 * acc2_smooth + 0.01 * acc2\n\n            net_txt.zero_grad()\n            loss.backward()\n            optim_txt.step()\n\n            writer.add_scalar('train/loss1', loss1.item(), iter_num)\n            writer.add_scalar('train/loss2', loss2.item(), iter_num)\n            writer.add_scalar('train/acc1', acc1, iter_num)\n            writer.add_scalar('train/acc2', acc2, iter_num)\n            writer.add_scalar('train/acc1_smooth', acc1_smooth, iter_num)\n            writer.add_scalar('train/acc2_smooth', acc2_smooth, iter_num)\n            writer.add_scalar('train/lr', sched_txt.get_lr()[0], iter_num)\n\n            if (iter_num % 100) == 0:\n                run_info = (\n                        'epoch: [{:3d}/{:3d}] | step: [{:4d}/{:4d}] | '\n                        'loss: {:.4f} | loss1: {:.4f} | loss2: {:.4f} | '\n                        'acc1: {:.2f} | acc2: {:.2f} | '\n                        'acc1_smooth: {:.3f} | acc2_smooth: {:.3f} | '\n                        'lr: {:.8f}'\n                ).format(epoch+1, 300, (i+1), loader_len,\n                        loss, loss1, loss2,\n                        acc1, acc2,\n                        acc1_smooth, acc2_smooth,\n                        sched_txt.get_lr()[0])\n                tqdm.write(run_info)\n\n        net_txt.eval()\n        tqdm.write('Saving checkpoint to: {}'.format(ckpt_path))\n        torch.save(net_txt.state_dict(), ckpt_path)\n\n        sched_txt.step()\n\n    writer.close()","metadata":{"id":"7uYsYpI6qoj5","execution":{"iopub.status.busy":"2024-03-04T09:37:21.225725Z","iopub.execute_input":"2024-03-04T09:37:21.226678Z","iopub.status.idle":"2024-03-04T09:37:21.242140Z","shell.execute_reply.started":"2024-03-04T09:37:21.226642Z","shell.execute_reply":"2024-03-04T09:37:21.241264Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"main()","metadata":{"id":"5Up88S2LrNCN","_kg_hide-output":false,"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/checkpoints\n!ls","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:21:50.270371Z","iopub.execute_input":"2024-03-04T11:21:50.271014Z","iopub.status.idle":"2024-03-04T11:21:51.256457Z","shell.execute_reply.started":"2024-03-04T11:21:50.270967Z","shell.execute_reply":"2024-03-04T11:21:51.255460Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"/kaggle/working/checkpoints\ncheckpoint_0.00040_True_train_2024_03_04_06_45_49.pth\ncheckpoint_0.00040_True_train_2024_03_04_09_37_24.pth\nevents.out.tfevents.1709534749.b76a8154565b.34.0\nevents.out.tfevents.1709545044.b76a8154565b.34.1\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:56:58.876355Z","iopub.execute_input":"2024-03-04T11:56:58.877322Z","iopub.status.idle":"2024-03-04T11:56:58.881577Z","shell.execute_reply.started":"2024-03-04T11:56:58.877285Z","shell.execute_reply":"2024-03-04T11:56:58.880561Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"FileLink(r'checkpoint_0.00040_True_train_2024_03_04_09_37_24.pth')","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:57:25.078848Z","iopub.execute_input":"2024-03-04T11:57:25.079498Z","iopub.status.idle":"2024-03-04T11:57:25.086693Z","shell.execute_reply.started":"2024-03-04T11:57:25.079468Z","shell.execute_reply":"2024-03-04T11:57:25.085641Z"},"trusted":true},"execution_count":80,"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/checkpoints/checkpoint_0.00040_True_train_2024_03_04_09_37_24.pth","text/html":"<a href='checkpoint_0.00040_True_train_2024_03_04_09_37_24.pth' target='_blank'>checkpoint_0.00040_True_train_2024_03_04_09_37_24.pth</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"%tensorboard --logdir /kaggle/working/checkpoints/checkpoint","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n    Encoder for preprocessed Caltech-UCSD Birds 200-2011 and Oxford 102\n    Category Flowers datasets, used in ``Learning Deep Representations of\n    Fine-grained Visual Descriptions``.\n\n    Warning: if you decide to not use all sentences (i.e., num_txts_eval > 0),\n    sentences will be randomly sampled and their features will be averaged to\n    provide a class representation. This means that the evaluation procedures\n    should be performed multiple times (using different seeds) to account for\n    this randomness.\n\n    Arguments:\n        net_txt (torch.nn.Module): text processing network.\n        net_img (torch.nn.Module): image processing network.\n        data_dir (string): path to directory containing dataset files.\n        split (string): which data split to load.\n        num_txts_eval (int): number of textual descriptions to use for each\n            class (0 = use all). The embeddings are averaged per-class.\n        batch_size (int): batch size to split data processing into chunks.\n        device (torch.device): which device to do computation in.\n\n    Returns:\n        cls_feats_img (list of torch.Tensor): list containing precomputed image\n            features for each image, separated by class.\n        cls_feats_txt (torch.Tensor): tensor containing precomputed (and\n            averaged) textual features for each class.\n        cls_list (list of string): list of class names.\n\"\"\"\ndef encode_data(net_txt, net_img, data_dir, split, num_txts_eval, batch_size, device):\n    cls_list = ['dresses','jackets and coats','jeans','pants','shirts','shorts','skirts','suits and blazers','sweaters','tops']\n\n    cls_feats_img = []\n    cls_feats_txt = []\n    for cls in cls_list:\n        # prepare image data\n        data_img_path = f'{data_dir}/img-emb/img-emb/val/{cls}.t7'\n        data_img = torch.Tensor(torch.load(data_img_path))\n        \n        feats_img = data_img[:, :].to(device)\n        if net_img is not None:\n            with torch.no_grad():\n                feats_img = net_img(feats_img)\n        cls_feats_img.append(feats_img)\n\n        # prepare text data\n        data_txt_path = f'{data_dir}/text-to-label/text-to-label/train/{cls}.t7'\n        data_txt = torch.LongTensor(torch.load(data_txt_path))\n\n        # select T texts from all instances to represent this class\n#         data_txt = data_txt.permute(0, 2, 1)\n#         total_txts = data_txt.size(0) * data_txt.size(1)\n#         data_txt = data_txt.contiguous().view(total_txts, -1)\n        if num_txts_eval > 0:\n            num_txts_eval = min(num_txts_eval, total_txts)\n            id_txts = torch.randperm(data_txt.size(0))[:num_txts_eval]\n            data_txt = data_txt[id_txts]\n\n        # convert to one-hot tensor to run through network\n        # TODO: adapt code to support batched version\n        txt_onehot = []\n        for txt in data_txt:\n            txt_onehot.append(labelvec_to_onehot(txt))\n        txt_onehot = torch.stack(txt_onehot)\n\n        # if we use a lot of text descriptions, it will not fit in gpu memory\n        # separate instances into mini-batches to process them using gpu\n        feats_txt = []\n        for batch in torch.split(txt_onehot, batch_size, dim=0):\n            with torch.no_grad():\n                out = net_txt(batch.to(device))\n            feats_txt.append(out)\n\n        # average the outputs\n        feats_txt = torch.cat(feats_txt, dim=0).mean(dim=0)\n        cls_feats_txt.append(feats_txt)\n\n    cls_feats_txt = torch.stack(cls_feats_txt, dim=0)\n\n    return cls_feats_img, cls_feats_txt, cls_list\n","metadata":{"id":"ydSz_P30Z2Bi","execution":{"iopub.status.busy":"2024-03-04T12:17:52.406336Z","iopub.execute_input":"2024-03-04T12:17:52.407279Z","iopub.status.idle":"2024-03-04T12:17:52.419682Z","shell.execute_reply.started":"2024-03-04T12:17:52.407244Z","shell.execute_reply":"2024-03-04T12:17:52.418593Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"  '''\n    Classification evaluation.\n\n    Arguments:\n        cls_feats_img (list of torch.Tensor): list containing precomputed image\n            features for each image, separated by class.\n        cls_feats_txt (torch.Tensor): tensor containing precomputed (and\n            averaged) textual features for each class.\n        cls_list (list of string): list of class names.\n\n    Returns:\n        avg_acc (float): percentage of correct classifications for all classes.\n        cls_stats (OrderedDict): dictionary whose keys are class names and each\n            entry is a dictionary containing the 'total' of images for the\n            class and the number of 'correct' classifications.\n    '''\ndef eval_classify(cls_feats_img, cls_feats_txt, cls_list):\n    cls_stats = OrderedDict()\n    for i, cls in enumerate(cls_list):\n        feats_img = cls_feats_img[i]\n        scores = torch.matmul(feats_img, cls_feats_txt.t())\n        max_ids = torch.argmax(scores, dim=1).to('cpu')\n        ground_truths = torch.LongTensor(scores.size(0)).fill_(i)\n        num_correct = (max_ids == ground_truths).sum().item()\n        cls_stats[cls] = {'correct': num_correct, 'total': ground_truths.size(0)}\n\n    total = sum([stats['total'] for _, stats in cls_stats.items()])\n    total_correct = sum([stats['correct'] for _, stats in cls_stats.items()])\n    avg_acc = total_correct / total\n    return avg_acc, cls_stats","metadata":{"id":"-FtnJsBVaR1Z","execution":{"iopub.status.busy":"2024-03-04T12:17:52.984497Z","iopub.execute_input":"2024-03-04T12:17:52.985393Z","iopub.status.idle":"2024-03-04T12:17:52.993312Z","shell.execute_reply.started":"2024-03-04T12:17:52.985360Z","shell.execute_reply":"2024-03-04T12:17:52.992333Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"def eval_retrieval(cls_feats_img, cls_feats_txt, cls_list, k_values=[1,5,10,50]):\n    '''\n    Retrieval evaluation (Average Precision).\n\n    Arguments:\n        cls_feats_img (list of torch.Tensor): list containing precomputed image\n            features for each image, separated by class.\n        cls_feats_txt (torch.Tensor): tensor containing precomputed (and\n            averaged) textual features for each class.\n        cls_list (list of string): list of class names.\n        k_values (list, optional): list of k-values to use for evaluation.\n\n    Returns:\n        map_at_k (OrderedDict): dictionary whose keys are the k_values and the\n            values are the mean Average Precision (mAP) for all classes.\n        cls_stats (OrderedDict): dictionary whose keys are class names and each\n            entry is a dictionary whose keys are the k_values and the values\n            are the Average Precision (AP) per class.\n    '''\n    total_num_cls = cls_feats_txt.size(0)\n    total_num_img = sum([feats.size(0) for feats in cls_feats_img])\n    scores = torch.zeros(total_num_cls, total_num_img)\n    matches = torch.zeros(total_num_cls, total_num_img)\n\n    for i, cls in enumerate(cls_list):\n        start_id = 0\n        for j, feats_img in enumerate(cls_feats_img):\n            end_id = start_id + feats_img.size(0)\n            scores[i, start_id:end_id] = torch.matmul(feats_img, cls_feats_txt[i])\n            if i == j: matches[i, start_id:end_id] = 1\n            start_id = start_id + feats_img.size(0)\n\n    for i, s in enumerate(scores):\n        _, inds = torch.sort(s, descending=True)\n        matches[i] = matches[i, inds]\n\n    map_at_k = OrderedDict()\n    for k in k_values:\n        map_at_k[k] = torch.mean(matches[:, 0:k]).item()\n\n    cls_stats = OrderedDict()\n    for i, cls in enumerate(cls_list):\n        ap_at_k = OrderedDict()\n        for k in k_values:\n            ap_at_k[k] = torch.mean(matches[i, 0:k]).item()\n        cls_stats[cls] = ap_at_k\n\n    return map_at_k, cls_stats","metadata":{"id":"e8NhyKIAanNo","execution":{"iopub.status.busy":"2024-03-04T12:17:53.561364Z","iopub.execute_input":"2024-03-04T12:17:53.562204Z","iopub.status.idle":"2024-03-04T12:17:53.572812Z","shell.execute_reply.started":"2024-03-04T12:17:53.562170Z","shell.execute_reply":"2024-03-04T12:17:53.571913Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"import pickle","metadata":{"execution":{"iopub.status.busy":"2024-03-04T12:02:03.642075Z","iopub.execute_input":"2024-03-04T12:02:03.642746Z","iopub.status.idle":"2024-03-04T12:02:03.646785Z","shell.execute_reply.started":"2024-03-04T12:02:03.642711Z","shell.execute_reply":"2024-03-04T12:02:03.645838Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"def evaluate():\n    rng_init(42)\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    net_txt = char_cnn_rnn()\n    model_path='/kaggle/working/checkpoints/checkpoint_0.00040_True_train_2024_03_04_09_37_24.pth'\n    net_txt.load_state_dict(torch.load(model_path, map_location=device))\n#     with open('/kaggle/working/utils/model.pickle','wb') as f1:\n#         pickle.dump(net_txt,f1)\n#     f1.close()\n    \n#     with open('/kaggle/working/utils/weights.pickle','wb') as f2:\n#         pickle.dump(torch.load(model_path),f2)\n#     f2.close()\n    print('Loaded')\n    net_txt = net_txt.to(device)\n    net_txt.eval()\n    \n    data_dir='/kaggle/input/embeddings'\n    cls_feats_img, cls_feats_txt, cls_list = encode_data(net_txt, None, data_dir,\n            'train', 0, 40, device)\n\n    mean_ap, cls_stats = eval_retrieval(cls_feats_img, cls_feats_txt, cls_list)\n    print('----- RETRIEVAL -----')\n        \n    print('  PER CLASS:')\n    for name, stats in cls_stats.items():\n        print(name)\n        for k, ap in stats.items():\n            print('{:.4f}: AP@{}'.format(ap, k))\n    print()\n\n    print('  mAP:')\n    for k, v in mean_ap.items():\n        print('{:.4f}: mAP@{}'.format(v, k))\n    print('---------------------')\n    print()\n\n    avg_acc, cls_stats = eval_classify(cls_feats_img, cls_feats_txt, cls_list)\n    print('--- CLASSIFICATION --')\n    \n    print('  PER CLASS:')\n    for name, stats in cls_stats.items():\n        print('{:.4f}: {}'.format(stats['correct'] / stats['total'], name))\n    print()\n\n    print('Average top-1 accuracy: {:.4f}'.format(avg_acc))\n    print('---------------------')\n    \n    \n    print()\n    with open('/kaggle/working/utils/char-CNN-RNN-embeddings-val.pickle','wb') as f3:\n        pickle.dump(cls_feats_txt,f3)\n    f3.close()","metadata":{"id":"unEfU6nFausl","execution":{"iopub.status.busy":"2024-03-04T12:17:57.411935Z","iopub.execute_input":"2024-03-04T12:17:57.412634Z","iopub.status.idle":"2024-03-04T12:17:57.423338Z","shell.execute_reply.started":"2024-03-04T12:17:57.412601Z","shell.execute_reply":"2024-03-04T12:17:57.422330Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"evaluate()","metadata":{"id":"2pjsVcyKa4Gf","execution":{"iopub.status.busy":"2024-03-04T12:18:02.240260Z","iopub.execute_input":"2024-03-04T12:18:02.241140Z","iopub.status.idle":"2024-03-04T12:18:27.407446Z","shell.execute_reply.started":"2024-03-04T12:18:02.241106Z","shell.execute_reply":"2024-03-04T12:18:27.406563Z"},"trusted":true},"execution_count":99,"outputs":[{"name":"stdout","text":"Loaded\n----- RETRIEVAL -----\n  PER CLASS:\ndresses\n0.0000: AP@1\n0.0000: AP@5\n0.1000: AP@10\n0.1000: AP@50\njackets and coats\n1.0000: AP@1\n1.0000: AP@5\n1.0000: AP@10\n0.9200: AP@50\njeans\n1.0000: AP@1\n1.0000: AP@5\n1.0000: AP@10\n0.9600: AP@50\npants\n1.0000: AP@1\n1.0000: AP@5\n1.0000: AP@10\n1.0000: AP@50\nshirts\n1.0000: AP@1\n1.0000: AP@5\n1.0000: AP@10\n0.8200: AP@50\nshorts\n1.0000: AP@1\n0.6000: AP@5\n0.4000: AP@10\n0.3400: AP@50\nskirts\n0.0000: AP@1\n0.4000: AP@5\n0.5000: AP@10\n0.4000: AP@50\nsuits and blazers\n0.0000: AP@1\n0.6000: AP@5\n0.7000: AP@10\n0.6200: AP@50\nsweaters\n1.0000: AP@1\n1.0000: AP@5\n1.0000: AP@10\n0.9000: AP@50\ntops\n1.0000: AP@1\n1.0000: AP@5\n1.0000: AP@10\n0.9800: AP@50\n\n  mAP:\n0.7000: mAP@1\n0.7600: mAP@5\n0.7700: mAP@10\n0.7040: mAP@50\n---------------------\n\n--- CLASSIFICATION --\n  PER CLASS:\n0.7855: dresses\n0.3290: jackets and coats\n0.6893: jeans\n0.8989: pants\n0.8200: shirts\n0.0513: shorts\n0.2191: skirts\n0.7148: suits and blazers\n0.7161: sweaters\n0.5400: tops\n\nAverage top-1 accuracy: 0.5971\n---------------------\n\n","output_type":"stream"}]},{"cell_type":"code","source":"os.makedirs('/kaggle/working/utils', exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T12:12:55.163386Z","iopub.execute_input":"2024-03-04T12:12:55.164379Z","iopub.status.idle":"2024-03-04T12:12:55.169045Z","shell.execute_reply.started":"2024-03-04T12:12:55.164342Z","shell.execute_reply":"2024-03-04T12:12:55.168066Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"!zip -r utils.zip /kaggle/working/utils","metadata":{"execution":{"iopub.status.busy":"2024-03-04T12:21:43.878235Z","iopub.execute_input":"2024-03-04T12:21:43.879189Z","iopub.status.idle":"2024-03-04T12:21:46.637404Z","shell.execute_reply.started":"2024-03-04T12:21:43.879145Z","shell.execute_reply":"2024-03-04T12:21:46.636392Z"},"trusted":true},"execution_count":104,"outputs":[{"name":"stdout","text":"updating: kaggle/working/utils/ (stored 0%)\nupdating: kaggle/working/utils/char-CNN-RNN-embeddings-val.pickle (deflated 7%)\nupdating: kaggle/working/utils/char-CNN-RNN-embeddings.pickle (deflated 7%)\nupdating: kaggle/working/utils/weights.pickle (deflated 8%)\nupdating: kaggle/working/utils/model.pickle (deflated 8%)\n","output_type":"stream"}]},{"cell_type":"code","source":"from IPython.display import FileLink","metadata":{"execution":{"iopub.status.busy":"2024-03-04T12:20:51.970934Z","iopub.execute_input":"2024-03-04T12:20:51.971345Z","iopub.status.idle":"2024-03-04T12:20:51.976269Z","shell.execute_reply.started":"2024-03-04T12:20:51.971307Z","shell.execute_reply":"2024-03-04T12:20:51.975290Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"FileLink('utils.zip')","metadata":{"execution":{"iopub.status.busy":"2024-03-04T12:21:05.861076Z","iopub.execute_input":"2024-03-04T12:21:05.861420Z","iopub.status.idle":"2024-03-04T12:21:05.867525Z","shell.execute_reply.started":"2024-03-04T12:21:05.861393Z","shell.execute_reply":"2024-03-04T12:21:05.866667Z"},"trusted":true},"execution_count":103,"outputs":[{"execution_count":103,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/checkpoints/utils.zip","text/html":"<a href='utils.zip' target='_blank'>utils.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}